{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Notebook - Goals - FOR EDINA\n",
    "\n",
    "**What?:**\n",
    "- Standard classification method example/tutorial\n",
    "\n",
    "**Who?:**\n",
    "- Researchers in ML\n",
    "- Students in computer science\n",
    "- Teachers in ML/STEM\n",
    "\n",
    "**Why?:**\n",
    "- Demonstrate capability/simplicity of core scipy stack. \n",
    "- Demonstrate common ML concept known to learners and used by researchers.\n",
    "\n",
    "**Noteable features to exploit:**\n",
    "- use of pre-installed libraries: <code>numpy</code>, <code>scikit-learn</code>, <code>matplotlib</code>\n",
    "\n",
    "**How?:**\n",
    "- clear to understand - minimise assumed knowledge\n",
    "- clear visualisations - concise explanations\n",
    "- recognisable/familiar - use standard methods\n",
    "- Effective use of core libraries\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification - K nearest neighbours\n",
    "\n",
    "K nearest neighbours is a simple and effective way to deal with classification problems. This method classifies each sample based on the class of the points that are closest to it.\n",
    "\n",
    "This is a supervised learning method, meaning that data used contains information on some feature that the model should predict.\n",
    "\n",
    "This notebook shows the process of classifying handwritten digits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Import libraries\n",
    "\n",
    "On Noteable, all the libaries required for this notebook are pre-installed, so they simply need to be imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import sklearn.datasets as ds\n",
    "import sklearn.model_selection as ms \n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import neighbors\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Data - Handwritten Digits\n",
    "\n",
    "In terms of data, [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html) has a loading function for some data regarding hand written digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the digits data from scikit into the notebook\n",
    "digits = ds.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell above loads the data as a [bunch object](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html), meaning that the data (in this case images of handwritten digits) and the target (the number that is written) can be split by accessing the attributes of the bunch object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store data and targets seperately\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "print(\"The data is of the shape\", X.shape)\n",
    "print(\"The target data is of the shape\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual samples in the <code>X</code> array each represent an image. In this representation, 64 numbers are used to represent a greyscale value on an 8\\*8 square. The images can be examined by using pyplot's [matshow](https://matplotlib.org/3.3.0/api/_as_gen/matplotlib.pyplot.matshow.html) function.\n",
    "\n",
    "The next cell displays the 17th sample in the dataset as an 8\\*8 image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure to display the 17th sample\n",
    "fig = plt.matshow(digits.images[17], cmap=plt.cm.gray)\n",
    "fig.axes.get_xaxis().set_visible(False)\n",
    "fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose instead of viewing the 17th sample, we want to see the average of samples corresponding to a certain value.\n",
    "\n",
    "This can be done as follows (using 0 as an example):\n",
    "- All samples where the target value is 0 are located\n",
    "- The mean of these samples is taken\n",
    "- The resulting 64 long array is reshaped to be 8\\*8 (for display)\n",
    "- The image is displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take samples with target=0\n",
    "izeros = np.where(y == 0)\n",
    "# take average across samples, reshape to visualise\n",
    "zeros = np.mean(X[izeros], axis=0).reshape(8,8)\n",
    "\n",
    "# display\n",
    "fig = plt.matshow(zeros, cmap=plt.cm.gray)\n",
    "fig.axes.get_xaxis().set_visible(False)\n",
    "fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Fit and test the model\n",
    "\n",
    "## Split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have an understanding of the data, the model can be fitted.\n",
    "\n",
    "Fitting the model involves setting some of the data aside for testing, and allowing the model to \"see\" the target values corresponding to the training samples.\n",
    "\n",
    "Once the model has been fitted to the training data, the model will be tested on some data it has not seen before. \n",
    "\n",
    "The next cell uses [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to shuffle all data, then set some data aside for testing later. \n",
    "\n",
    "For this example, $\\frac{1}{4}$ of the data will be set aside for testing, and the model will be trained on the remaining training set.\n",
    "\n",
    "As before, <code>X</code> corresponds to data samples, and <code>y</code> corresponds to labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data to train and test sets\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    ms.train_test_split(X, y, test_size=0.25, shuffle=True,\n",
    "                       random_state=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data can be examined - here you can see that 1347 samples have been put into the training set, and 450 have been set aside for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print shape of data\n",
    "print(\"training samples:\", X_train.shape)  \n",
    "print(\"testing samples :\", X_test.shape)\n",
    "print(\"training targets:\", y_train.shape) \n",
    "print(\"testing targets :\", y_test.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PCA to visualise data\n",
    "\n",
    "Before diving into classifying, it is useful to visualise the data.\n",
    "\n",
    "Since each sample has 64 dimensions, some dimensionality reduction is needed in order to visualise the samples as points on a 2D map.\n",
    "\n",
    "One of the easiest ways of visualising high dimensional data is by principal component analysis (PCA). This maps the 64 dimensional image data onto a lower dimension map (here we will map to 2D) so it can be easily viewed on a screen.\n",
    "\n",
    "In this case, the 2 most important \"components\" are maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create PCA model with 2 components\n",
    "pca = decomposition.PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to perform the PCA on the samples, and store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform training data to 2 principal components\n",
    "X_pca = pca.fit_transform(X_train)\n",
    "\n",
    "# transform test data to 2 principal components\n",
    "T_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shape of result\n",
    "print(X_pca.shape) \n",
    "print(T_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the above cell, the <code>X_pca</code> and <code>T_pca</code> data is now represented by only 2 elements per sample. The number of samples has remained the same.\n",
    "\n",
    "Now that there is a 2D representation of the data, it can be plotted on a regular scatter graph. Since the labels corresponding to each point are stored in the <code>y_train</code> variable, the plot can be colour coded by target value!\n",
    "\n",
    "Different coloured dots have different target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the colours for each digit\n",
    "cmap_digits = plt.cm.tab10\n",
    "\n",
    "# plot training data with labels\n",
    "plt.figure(figsize = (9,6))\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], s=7, c=y_train,\n",
    "            cmap=cmap_digits, alpha=0.7)\n",
    "plt.title(\"Training data coloured by target value\")\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and fit the model\n",
    "\n",
    "The scikit-learn library allows fitting of a k-NN model just as with PCA above.\n",
    "\n",
    "First, create the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "knn = neighbors.KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step fits the k-NN model using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model to training data\n",
    "knn.fit(X_train,y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model\n",
    "\n",
    "Now use the data that was set aside earlier - this stage involves getting the model to \"guess\" the samples (this time without seeing their target values).\n",
    "\n",
    "Once the model has predicted the sample's class, a score can be calculated by checking how many samples the model guessed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test data\n",
    "preds = knn.predict(X_test)\n",
    "\n",
    "# test model on test data\n",
    "score = round(knn.score(X_test,y_test)*100, 2)\n",
    "print(\"Score on test data: \" + str(score) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "98.44% is a really high score, one that would not likely be seen on real life applications of the method.\n",
    "\n",
    "It can often be useful to visualise the results of your example. Below are plots showing:\n",
    "- The labels that the model predicted for the test data\n",
    "- The actual labels for the test data\n",
    "- The data points that were incorrectly labelled\n",
    "\n",
    "In this case, the predicted and actual plots are very similar, so these plots are not very informative. In other cases, this kind of visualisation may reveal patterns for you to explore further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 3 axes\n",
    "fig, axes = plt.subplots(2,2,figsize=(12,12))\n",
    "\n",
    "# top left axis for predictions\n",
    "axes[0,0].scatter(T_pca[:,0], T_pca[:,1], s=5, \n",
    "                  c=preds, cmap=cmap_digits)\n",
    "axes[0,0].set_title(\"Predicted labels\")\n",
    "\n",
    "# top right axis for actual targets\n",
    "axes[0,1].scatter(T_pca[:,0], T_pca[:,1], s=5, \n",
    "                  c=y_test, cmap=cmap_digits)\n",
    "axes[0,1].set_title(\"Actual labels\")\n",
    "\n",
    "# bottom left axis coloured to show correct and incorrect\n",
    "axes[1,0].scatter(T_pca[:,0], T_pca[:,1], s=5, \n",
    "                  c=(preds==y_test))\n",
    "axes[1,0].set_title(\"Incorrect labels\")\n",
    "\n",
    "# bottom right axis not used\n",
    "axes[1,1].set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So which samples did the model get wrong?\n",
    "\n",
    "There were 7 samples that were misclassified. These can be displayed alongside their actual and predicted labels using the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the misclassified samples\n",
    "misclass = np.where(preds!=y_test)[0]\n",
    "\n",
    "# display misclassified samples\n",
    "r, c = 1, len(misclass)\n",
    "fig, axes = plt.subplots(r,c,figsize=(10,5))\n",
    "\n",
    "for i in range(c):\n",
    "    ax = axes[i]\n",
    "    ax.matshow(X_test[misclass[i]].reshape(8,8),cmap=plt.cm.gray)\n",
    "    ax.set_axis_off()\n",
    "    act = y_test[misclass[i]]\n",
    "    pre = preds[misclass[i]]\n",
    "    strng = \"actual: {a:.0f} \\npredicted: {p:.0f}\".format(a=act, p=pre)\n",
    "    ax.set_title(strng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, a confusion matrix can be used to identify which samples are misclassified by the model. This can help you identify if their are samples that are commonly misidentified - for example you may identify that 8's are often mistook for 1's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "conf = metrics.confusion_matrix(y_test,preds)\n",
    "\n",
    "# figure\n",
    "f, ax = plt.subplots(figsize=(9,5))\n",
    "im = ax.imshow(conf, cmap=plt.cm.RdBu)\n",
    "\n",
    "# set labels as ticks on axes\n",
    "ax.set_xticks(np.arange(10))\n",
    "ax.set_yticks(np.arange(10))\n",
    "ax.set_xticklabels(list(range(0,10)))\n",
    "ax.set_yticklabels(list(range(0,10)))\n",
    "ax.set_ylim(9.5,-0.5)\n",
    "\n",
    "# axes labels\n",
    "ax.set_ylabel(\"actual value\")\n",
    "ax.set_xlabel(\"predicted value\")\n",
    "ax.set_title(\"Digit classification confusion matrix\")\n",
    "\n",
    "# display\n",
    "plt.colorbar(im).set_label(label=\"number of classifications\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
