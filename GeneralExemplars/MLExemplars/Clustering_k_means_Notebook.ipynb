{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Notebook - Goals - FOR EDINA\n",
    "\n",
    "**What?:**\n",
    "- K-means visualisations of generated data.\n",
    "- Interactive visualisation.\n",
    "- Discussion of issues with k-means: elbow method and non-linearly separable data.\n",
    "\n",
    "**Who?:**\n",
    "- intro to ML course (3rd year computer science UG).\n",
    "- PG students in social sciences learning ML.\n",
    "- Increasingly used in non-computer science fields.\n",
    "\n",
    "**Why?:**\n",
    "- Common topic, often used in teaching and research.\n",
    "- Mandatory subject for students at many universities.\n",
    "\n",
    "**Noteable features to exploit:**\n",
    "- Visualisation\n",
    "- Interactive visualisation libraries (bokeh),\n",
    "- Tutorial format\n",
    "- Embedded latex\n",
    "- Pre-installed library use\n",
    "\n",
    "**How? Tools/methods used:**\n",
    "- <code>bokeh</code> - interactive visualisation\n",
    "- embedded latex\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means clustering\n",
    "\n",
    "K-means is a simple and common clustering algorithm. \n",
    "\n",
    "It is unsupervised, meaning that labels are not used to train the model on \"known\" data. Instead, the algorithm uses the properties of the data points (*samples*) to group each point into clusters.\n",
    "\n",
    "### Uses of K-means\n",
    "\n",
    "K-means is used to cluser data that does not have explicit labels. This can help give an intuition about what is going on within the data.\n",
    "\n",
    "K-means is additionally used as a dimension reduction technique, whereby the clustering given by the algorithm is used to represent data in further processing.\n",
    "\n",
    "### Goal - What is a successful clustering?\n",
    "\n",
    "K-means is a polythetic method, meaning that it aims to group points that are similar to one another. The alternative to this is monothetic methods, which cluster groups based on some common property such as age.\n",
    "\n",
    "K-means assumes that a good cluster has the following properties:\n",
    "- The centre of the cluster *(centroid)* is the mean average of the points in that cluster.\n",
    "- Each point in the cluster is closer to that cluster's centroid than any other cluster's centroid.\n",
    "\n",
    "Based on this idea of success, the goal is to minimise the aggregate intra-cluster distance. This objective function takes the following form: sum the distances between each point, $x_i$ and it's assigned cluster centre, $c_j$:\n",
    "\n",
    "$$\n",
    "J = \\sum_{j=1}^{K} \\sum_{i=1}^{N} ‖ \\mathbf{x}_i^{(j)} - {\\mu}_j ‖^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $K$ is the number of clusters, chosen in advance,\n",
    "- ${x}_i$ is the $i$'th data point in the set, and \n",
    "- ${\\mu}_j$ is the centroid for cluster $j$\n",
    "\n",
    "### Algorithm - How does it work?\n",
    "\n",
    "<strong>K-means input:</strong>\n",
    "- $N$ instances of d-dimensional data\n",
    "- $K$ - the number of populations that are to be extracted from the data.\n",
    "\n",
    "Whilst there is scope to use alternative distance measures, Euclidean distance will be used in this tutorial.\n",
    "\n",
    "<strong>K-means general approach:</strong>\n",
    "- Initialise $K$ centroids randomly\n",
    "- Iterate the following until convergence (no change in centroids):\n",
    "    - For each data point, $x_i$ in data set, compute distance to every centroid. Assign point to cluster with nearest centroid.\n",
    "    - Compute new centroids, ${\\mu}_j$ by taking a mean average of all points now in that cluster.\n",
    "    \n",
    "Once the centroids stop changing, the objective function (above) can be calculated.\n",
    "    \n",
    "<i><strong>Note:</strong> Notice that the cluster centres are initialised <strong>randomly</strong>. This means that running the algorithm with a different random state will yield different results. For this reason, this tutorial sets the random state in every function that involves randomness, which means the results will not change each time the notebook is run. If using k-means in practise, it is almost always necessary to run the algorithm a few times with different random states in order to attain a comprehensive picture of results.</i>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing useful libraries\n",
    "\n",
    "The following cell loads some useful python code to use later. These are all pre-installed on Noteable, so they just need to be loaded into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import sklearn.datasets as ds\n",
    "import sklearn.cluster as cl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# magic iPython function\n",
    "%matplotlib inline    \n",
    "\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook    \n",
    "import bokeh\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", color_codes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Data\n",
    "\n",
    "To show the results of the algorithm, some data is needed for practice! [Scikit-learn](https://scikit-learn.org/stable/) provides some useful functions for generating pretend data.\n",
    "\n",
    "The function provides data (here stored in variable <code>X</code>) and labels (stored in variable <code>y</code>). Because this is an unsupervised learning method, the labels won't be used to assign clusters, but will come in handy later when the model performance is assessed. Additionally, a random state is used for reproducibility.\n",
    "\n",
    "The data generated will be 2D, with 1000 data points in 4 \"blobs\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate pretend \"blob\" data\n",
    "X, y = ds.make_blobs(n_samples=1000, centers=4, \n",
    "                          cluster_std=0.7, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this data is only 2D, it can be plotted for visualisation.\n",
    "\n",
    "One of the useful visualisation libraries included in the Noteable service is [bokeh](https://bokeh.org). The functions included in this library allow for interactive graphs within the notebook.\n",
    "\n",
    "The cell below plots the data (in rainbow colours, because we can). You can drag the resultant graph about, zoom in and out and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell bokeh to plot to the notebook\n",
    "output_notebook()\n",
    "\n",
    "# define rainbow colours for plot\n",
    "xcols = np.random.random(size=1000) * 500\n",
    "ycols = np.random.random(size=1000) * 500\n",
    "colors = [\n",
    "    \"#%02x%02x%02x\" % (int(r), int(g), 150) for r, g in \n",
    "    zip(xcols, ycols)\n",
    "]\n",
    "\n",
    "# specify figure and title\n",
    "p = figure( plot_width=450, plot_height=350)\n",
    "p.circle(X[:,0], X[:,1], radius=0.05, fill_color=colors, \n",
    "         fill_alpha=0.8, line_color=None)\n",
    "p.title.text = \"Raw blobs data\"\n",
    "p.title.align = \"center\"\n",
    "\n",
    "# plot in notebook (interactive)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Run K-Means\n",
    "\n",
    "Time to put k-means to the test, and see if the algorithm can pick out the groups in the underlying data.\n",
    "\n",
    "K-means requires a $K$ value as input, so the number of clusters desired needs to be selected up front. Here, it is sensible to pick $K=4$ since it is clear from the data above that the points fall into 4 clusters.\n",
    "\n",
    "The next cell uses scikit-learn's [k-means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) clustering algorithm to sort the data into 4 clusters, then prints out the resulting clusters. \n",
    "\n",
    "The larger black hexagons on the graph are the cluster centroids for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model - note we choose k=4 and set a random state\n",
    "kmeans = cl.KMeans(4, random_state=100)\n",
    "\n",
    "# fit model to blobs data\n",
    "kmeans.fit(X)\n",
    "\n",
    "# store labels and centroids\n",
    "Y  = kmeans.labels_.astype(np.int) \n",
    "mu = kmeans.cluster_centers_ \n",
    "\n",
    "# specify colours for clusters\n",
    "four_colors = np.array([x for x in ('#c22980', '#6d48d8', '#fabb1b', '#0ff')])\n",
    "\n",
    "# tell bokeh to plot to the notebook\n",
    "output_notebook()\n",
    "\n",
    "# specify figure and title\n",
    "p = figure(plot_width=450, plot_height=350)\n",
    "\n",
    "# plots different colors for different clusters\n",
    "p.circle(X[:,0], X[:,1], radius=0.05, \n",
    "         color=four_colors[Y].tolist(), fill_alpha=0.5, \n",
    "         line_color=None)\n",
    "# Plot centroids of clusters as black hexagons\n",
    "p.hex(mu[:,0], mu[:,1], size=7, color=\"black\")\n",
    "\n",
    "p.title.text = \"Clusters with centroids\"\n",
    "p.title.align = \"center\"\n",
    "\n",
    "# plot in notebook (interactive)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like k-means did a reasonable job! Whilst a graph is useful for this example, it can sometimes be important to gather other performance statistics such as accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to calculate accuracy\n",
    "def acc(y,y_pred):\n",
    "    correct = np.sum(y == y_pred)\n",
    "    incorrect = np.sum(y != y_pred)\n",
    "    accuracy = correct / (correct+incorrect)\n",
    "    print(\"Accuracy is: \" + str(accuracy*100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find accuracy in this instance\n",
    "acc(y,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may seem low given the graph looks good. But bear in mind that there are no strict labels - the numbers associated with each cluster are intended as a way to group each sample, not make any claim about the characteristics of the group itself.\n",
    "\n",
    "In other words, the number associated with each group may change (try changing the random state when setting up the model). Suppose the groupings are identical, but the numbers differ between the original labels and the labels assigned in the model. \n",
    "\n",
    "This could be challenging to resolve, but in this case there are only 4 clusters  which can be easily visualised. \n",
    "\n",
    "The following plot demonstrates this. Points that have been assigned the same label as the original data are represented by green markers, and points that have different labels are represented by red markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take intersect of prediction and ground truth\n",
    "correct_vector = 1*(np.equal(y,Y))\n",
    "\n",
    "# tell bokeh to plot to the notebook\n",
    "output_notebook()\n",
    "\n",
    "# specify colours for clusters\n",
    "colors2 = np.array([x for x in ('red', 'green')])\n",
    "\n",
    "# specify figure\n",
    "p = figure(plot_width=450, plot_height=350)\n",
    "# plots different colors for different clusters\n",
    "p.circle(X[:,0], X[:,1], radius=0.05, \n",
    "         color=colors2[correct_vector].tolist(), fill_alpha=0.5, \n",
    "         line_color=None)\n",
    "\n",
    "# plot centroids of clusters as black hexagons\n",
    "p.hex(mu[:,0], mu[:,1], size=7, color=\"black\")\n",
    "\n",
    "p.title.text = \"k-means labels against original labels\"\n",
    "p.title.align = \"center\"\n",
    "\n",
    "# plot in notebook (interactive)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the uppermost clusters have been assigned different labels to the lower clusters.\n",
    "\n",
    "The label numbers can be deduced by referring to the colour palette used in the visualisations above.\n",
    "\n",
    "The line of code corresponding with labels 0 to three is defined in the line of code:\n",
    "\n",
    "<code>four_colors = np.array([x for x in ('#c22980', '#6d48d8', '#fabb1b', '#0ff')])</code>\n",
    "\n",
    "Each of these are hexadecimal value codes for the colours:\n",
    "<p style=\"color:#c22980\">#c22980 - pink - label \"0\"</p>\n",
    "<p style=\"color:#6d48d8\">#6d48d8 - purple - label \"1\"</p>\n",
    "<p style=\"color:#fabb1b\">#fabb1b - yellow - label \"2\"</p>\n",
    "<p style=\"color:#0ff\">#0ff - blue - label \"3\"</p>\n",
    "\n",
    "The cell below plots the labels as per this color scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell bokeh to plot to the notebook\n",
    "output_notebook()\n",
    "\n",
    "# specify colours for clusters\n",
    "colors = np.array([x for x in ('#c22980', \n",
    "                               '#6d48d8', '#fabb1b', '#0ff')])\n",
    "\n",
    "# specify figure for true labels\n",
    "left = figure(plot_width=350, plot_height=350)\n",
    "\n",
    "# plot with colours of true data\n",
    "left.circle(X[:,0], X[:,1], radius=0.05, \n",
    "         color=colors[y].tolist(), fill_alpha=0.5, \n",
    "         line_color=None)\n",
    "left.title.text = \"Labels given by k-means\"\n",
    "left.title.align = \"center\"\n",
    "\n",
    "# plot with colours of predicted data\n",
    "right = figure(plot_width=350, plot_height=350)\n",
    "right.circle(X[:,0], X[:,1], radius=0.05, \n",
    "         color=colors[Y].tolist(), fill_alpha=0.5, \n",
    "         line_color=None)\n",
    "right.title.text = \"Labels given in original data\"\n",
    "right.title.align = \"center\"\n",
    "\n",
    "# plot in notebook (interactive)\n",
    "p = bokeh.layouts.gridplot([[left,right]])\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the top two clusters are pink and blue in both, but that the colours are switched between the two graphs.\n",
    "\n",
    "The chart below shows the labels as deduced from the above graphs(from top to bottom):\n",
    "\n",
    "| cluster in image | true label | predicted label |\n",
    "|------------------|------------|-----------------|\n",
    "| top              | blue=3     | pink=0          |\n",
    "| second           | pink=0     | blue=3          |\n",
    "| third            | yellow=2   | yellow=2        |\n",
    "| bottom           | purple=1   | purple=1        |\n",
    "\n",
    "Therefore, to amend the problem and attain a new score, switch the labels 0 and 3 in the predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take copy of predictions\n",
    "new_pred = np.copy(Y)\n",
    "\n",
    "# find predictions labelled 0 and 3\n",
    "where_0s = np.where(new_pred==0)\n",
    "where_3s = np.where(new_pred==3)\n",
    "\n",
    "# switch 0 to 3, switch 3 to 0\n",
    "new_pred[where_0s] = 3\n",
    "new_pred[where_3s] = 0\n",
    "\n",
    "# print first few prediction labels to check as expected\n",
    "print(Y[:20])\n",
    "print(new_pred[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the colours of most data points should match up between the predicted and original labels. \n",
    "\n",
    "Visualise as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell bokeh to plot to the notebook\n",
    "output_notebook()\n",
    "\n",
    "# plot with colours of predicted data\n",
    "left = figure(plot_width=350, plot_height=350)\n",
    "left.circle(X[:,0], X[:,1], radius=0.05, \n",
    "         color=colors[new_pred].tolist(), fill_alpha=0.5, \n",
    "         line_color=None)\n",
    "left.title.text = \"Labels given by k-means - reassigned\"\n",
    "left.title.align = \"center\"\n",
    "\n",
    "# plot with colours of true data\n",
    "right = figure(plot_width=350, plot_height=350)\n",
    "right.circle(X[:,0], X[:,1], radius=0.05, \n",
    "         color=colors[y].tolist(), fill_alpha=0.5, \n",
    "         line_color=None)\n",
    "right.title.text = \"Labels given in original data\"\n",
    "right.title.align = \"center\"\n",
    "\n",
    "# plot in notebook (interactive)\n",
    "p = bokeh.layouts.gridplot([[left,right]])\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the colours of points match up much better!\n",
    "\n",
    "Calculate the score again (using the function defined above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score with reassigned prediction values\n",
    "acc(y,new_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy has gone up to 99.1%, a very high score!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Limitations of k-means\n",
    "\n",
    "## Non-linearly separable data\n",
    "\n",
    "Like all clustering algorithms, k-means handles some kinds of data more effectively than others. In the above example, the algorithm is used on data that clearly separates out into groups. But in data that isn't linearly seperable, it can misclassify.\n",
    "\n",
    "The next example generates some data in the shape of moons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate moon shaped data\n",
    "moon_data, moon_true = ds.make_moons(n_samples=200, noise=0.03, \n",
    "                                     random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot generated data using bokeh\n",
    "\n",
    "# tell bokeh to plot to the notebook\n",
    "output_notebook()\n",
    "\n",
    "# define rainbow colours for plot\n",
    "xcols = np.random.random(size=200) * 500\n",
    "ycols = np.random.random(size=200) * 500\n",
    "colors = [\n",
    "    \"#%02x%02x%02x\" % (int(r), int(g), 150) for r, g in \n",
    "    zip(xcols, ycols)\n",
    "]\n",
    "\n",
    "# specify figure and title\n",
    "p = figure(plot_width=450, plot_height=350)\n",
    "p.circle(moon_data[:,0], moon_data[:,1], \n",
    "         radius=0.025, fill_color=colors, \n",
    "         fill_alpha=0.8, line_color=None)\n",
    "\n",
    "p.title.text = \"Moon data\"\n",
    "p.title.align = \"center\"\n",
    "\n",
    "# plot in notebook (interactive)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply k-means to the new data. Again, generate a model and fit to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model - note we choose K=2 and set a random state\n",
    "kmeans = cl.KMeans(2, random_state=0)\n",
    "\n",
    "# fit model to data\n",
    "kmeans.fit(moon_data)\n",
    "\n",
    "# store the predicted labels and cluster centres\n",
    "Y = kmeans.labels_.astype(np.int)\n",
    "mu = kmeans.cluster_centers_\n",
    "\n",
    "# tell bokeh to plot to the notebook\n",
    "output_notebook()\n",
    "\n",
    "# specify colours for clusters\n",
    "colors = np.array([x for x in ('#c22980', \n",
    "                               '#6d48d8')])\n",
    "\n",
    "# specify figure and title\n",
    "p = figure(plot_width=450, plot_height=350)\n",
    "\n",
    "# plots different colors for different clusters\n",
    "p.circle(moon_data[:,0], moon_data[:,1], radius=0.025, \n",
    "         color=colors[Y].tolist(), fill_alpha=0.5, \n",
    "         line_color=None)\n",
    "\n",
    "# Plot centroids of clusters as black hexagons\n",
    "p.hex(mu[:,0], mu[:,1], size=7, color=\"black\")\n",
    "\n",
    "p.title.text = \"Moon data - classified\"\n",
    "p.title.align = \"center\"\n",
    "\n",
    "# plot in notebook (interactive)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem of picking K\n",
    "\n",
    "In the first example in this notebook, it was fairly obvious that 4 clusters were present in the data. In real life data, this is rarely the case. \n",
    "\n",
    "Additionally the objective function will improve the closer the value to $K$. Imagine the case where $K$ is the number of samples - the distance from each point to its centroid would be 0, since each point would be the same as it's centroid. But of course the number of points is massive, and the result will simply be clusters that are themselves the data points each with it's own label!\n",
    "\n",
    "So what is the best number of clusters to find in a data set? As a ballpark figure for basic exercises, 5 is often a good starting point. But we can do better. One way of finding a good first try figure is to try different values and apply the elbow method.\n",
    "\n",
    "The following code plots the value of the objective function for $K$ values from 1 to 10, when k-means is applied to the original blobs data.\n",
    "\n",
    "Note that the [inertia_](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) attribute is used to find the value of the objective function in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the k-means algorithm and get the index of data points clusters\n",
    "\n",
    "sse = []\n",
    "Ks = range(1, 11)\n",
    "\n",
    "# for K values 1 to 10\n",
    "for k in Ks:\n",
    "    # fit a kmeans model with k clusters on original data\n",
    "    km = cl.KMeans(n_clusters=k, random_state=0).fit(X)\n",
    "    # find value of objective function\n",
    "    sse.append(km.inertia_)\n",
    "\n",
    "# Plot sse against k\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(Ks, sse, '-m*')\n",
    "plt.xlabel('K-value used')\n",
    "plt.ylabel('Objective function value')\n",
    "plt.title(\"Elbow method using objective function\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the graph above, there is an \"elbow\" at $K=4$ where the curve flattens out. The objective function (which we want to minimise) does not improve much beyond $K=5$ (the graph flattens).\n",
    "\n",
    "Since using higher values of $K$ can be costly, $K=4$ is a sensible choice in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Closing remarks\n",
    "\n",
    "K-means provides a fairly intuitive way to group data, but has its limitations. This notebook has covered the basics of k-means visualisation and theory.\n",
    "\n",
    "As a follow on exercise, it may be helpful to attempt k-means on some data of your own. Scikit-learn has some [inbuilt datasets](https://scikit-learn.org/stable/datasets/index.html) which may be helpful in this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
